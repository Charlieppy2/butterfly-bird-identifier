"""
è¨“ç·´è…³æœ¬ï¼šçµæ§‹åŒ–è§£è®€ç”Ÿæˆæ¨¡å‹
ç”¨æ–¼è¨“ç·´ä¸€å€‹èƒ½å¤ æ ¹æ“šç‰©ç¨®å’Œé¡åˆ¥ç”Ÿæˆè§£è®€æ–‡æœ¬çš„æ¨¡å‹

è¨“ç·´æµç¨‹ï¼š
1. åŠ è¼‰å’Œé è™•ç†æ•¸æ“š
2. æº–å‚™æ•¸æ“šé›†å’Œæ•¸æ“šåŠ è¼‰å™¨
3. è¨­ç½®è¨“ç·´åƒæ•¸
4. è¨“ç·´å¾ªç’°ï¼ˆå‰å‘å‚³æ’­ã€æå¤±è¨ˆç®—ã€åå‘å‚³æ’­ï¼‰
5. è©•ä¼°ï¼ˆå›°æƒ‘åº¦ã€BLEUåˆ†æ•¸ï¼‰
6. ä¿å­˜æ¨¡å‹
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2LMHeadModel, GPT2Tokenizer, get_linear_schedule_with_warmup
from torch.optim import AdamW
import pandas as pd
import numpy as np
from tqdm import tqdm
import os
import json
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import warnings
warnings.filterwarnings('ignore')

# è¨­ç½®è¨­å‚™
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è¨­å‚™: {device}")

class InterpretationDataset(Dataset):
    """
    æ•¸æ“šé›†é¡
    ç”¨æ–¼åŠ è¼‰å’Œè™•ç†è¨“ç·´æ•¸æ“š
    """
    def __init__(self, csv_path, tokenizer, max_length=256):
        """
        åˆå§‹åŒ–æ•¸æ“šé›†
        
        Args:
            csv_path: CSVæ–‡ä»¶è·¯å¾‘
            tokenizer: åˆ†è©å™¨
            max_length: æœ€å¤§åºåˆ—é•·åº¦
        """
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # è®€å–CSVæ•¸æ“š
        df = pd.read_csv(csv_path)
        self.data = df.to_dict('records')
        
        print(f"âœ… åŠ è¼‰äº† {len(self.data)} æ¢è¨“ç·´æ•¸æ“š")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        """
        ç²å–ä¸€æ¢æ•¸æ“š
        
        Returns:
            input_ids: è¼¸å…¥ID
            attention_mask: æ³¨æ„åŠ›æ©ç¢¼
            labels: æ¨™ç±¤ï¼ˆç”¨æ–¼è¨ˆç®—æå¤±ï¼‰
        """
        item = self.data[idx]
        
        # æ§‹å»ºè¼¸å…¥ï¼šç‰©ç¨® + é¡åˆ¥
        species = item['species']
        category = item['category']
        interpretation = item['interpretation_text']
        
        # Input format: "{species}çš„{category}ï¼š{interpretation}"
        # Note: species name is in Chinese, but interpretation text is in English
        input_text = f"{species}çš„{category}ï¼š{interpretation}"
        
        # ä½¿ç”¨åˆ†è©å™¨ç·¨ç¢¼
        encoded = self.tokenizer.encode_plus(
            input_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        input_ids = encoded['input_ids'].squeeze()
        attention_mask = encoded['attention_mask'].squeeze()
        
        # æ¨™ç±¤å°±æ˜¯input_idsï¼ˆç”¨æ–¼è¨ˆç®—æå¤±ï¼‰
        labels = input_ids.clone()
        
        # å°‡paddingä½ç½®çš„æ¨™ç±¤è¨­ç‚º-100ï¼ˆæå¤±è¨ˆç®—æ™‚æœƒå¿½ç•¥ï¼‰
        labels[labels == self.tokenizer.pad_token_id] = -100
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels
        }

def calculate_perplexity(model, dataloader, device):
    """
    è¨ˆç®—å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰
    å›°æƒ‘åº¦è¶Šä½ï¼Œæ¨¡å‹è¶Šå¥½
    
    Args:
        model: æ¨¡å‹
        dataloader: æ•¸æ“šåŠ è¼‰å™¨
        device: è¨­å‚™
    
    Returns:
        perplexity: å›°æƒ‘åº¦å€¼
    """
    model.eval()
    total_loss = 0
    total_tokens = 0
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="è¨ˆç®—å›°æƒ‘åº¦"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            
            # è¨ˆç®—æœ‰æ•ˆtokenæ•¸é‡ï¼ˆæ’é™¤paddingï¼‰
            valid_tokens = (labels != -100).sum().item()
            total_loss += loss.item() * valid_tokens
            total_tokens += valid_tokens
    
    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')
    perplexity = torch.exp(torch.tensor(avg_loss)).item()
    
    return perplexity

def calculate_bleu(model, tokenizer, test_data, device, num_samples=10):
    """
    è¨ˆç®—BLEUåˆ†æ•¸
    BLEUåˆ†æ•¸è¶Šé«˜ï¼Œç”Ÿæˆæ–‡æœ¬è³ªé‡è¶Šå¥½
    
    Args:
        model: æ¨¡å‹
        tokenizer: åˆ†è©å™¨
        test_data: æ¸¬è©¦æ•¸æ“š
        device: è¨­å‚™
        num_samples: è¨ˆç®—BLEUçš„æ¨£æœ¬æ•¸é‡
    
    Returns:
        avg_bleu: å¹³å‡BLEUåˆ†æ•¸
    """
    model.eval()
    bleu_scores = []
    smoothing = SmoothingFunction().method1
    
    # éš¨æ©Ÿé¸æ“‡æ¨£æœ¬é€²è¡Œè©•ä¼°
    sample_indices = np.random.choice(len(test_data), min(num_samples, len(test_data)), replace=False)
    
    with torch.no_grad():
        for idx in sample_indices:
            item = test_data[idx]
            species = item['species']
            category = item['category']
            reference = item['interpretation_text']
            
            # ç”Ÿæˆæ–‡æœ¬
            input_text = f"{species}çš„{category}ï¼š"
            input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)
            
            # ç”Ÿæˆ
            output = model.generate(
                input_ids,
                max_length=256,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
            
            # è§£ç¢¼ç”Ÿæˆçš„æ–‡æœ¬
            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
            # ç§»é™¤è¼¸å…¥éƒ¨åˆ†ï¼Œåªä¿ç•™ç”Ÿæˆéƒ¨åˆ†
            generated_text = generated_text.replace(input_text, "").strip()
            
            # è¨ˆç®—BLEUåˆ†æ•¸
            reference_tokens = list(reference)
            generated_tokens = list(generated_text)
            
            try:
                bleu = sentence_bleu([reference_tokens], generated_tokens, smoothing_function=smoothing)
                bleu_scores.append(bleu)
            except:
                pass
    
    avg_bleu = np.mean(bleu_scores) if bleu_scores else 0.0
    return avg_bleu

def train_model(
    csv_path='training_data_interpretation.csv',
    output_dir='models/interpretation_model',
    num_epochs=5,
    batch_size=4,
    learning_rate=5e-5,
    max_length=256,
    warmup_steps=100
):
    """
    è¨“ç·´æ¨¡å‹çš„ä¸»å‡½æ•¸
    
    Args:
        csv_path: è¨“ç·´æ•¸æ“šCSVè·¯å¾‘
        output_dir: æ¨¡å‹ä¿å­˜ç›®éŒ„
        num_epochs: è¨“ç·´è¼ªæ•¸
        batch_size: æ‰¹æ¬¡å¤§å°
        learning_rate: å­¸ç¿’ç‡
        max_length: æœ€å¤§åºåˆ—é•·åº¦
        warmup_steps: é ç†±æ­¥æ•¸
    """
    print("=" * 60)
    print("ğŸš€ é–‹å§‹è¨“ç·´çµæ§‹åŒ–è§£è®€ç”Ÿæˆæ¨¡å‹")
    print("=" * 60)
    
    # 1. åŠ è¼‰æ¨¡å‹å’Œåˆ†è©å™¨
    print("\nğŸ“¥ æ­¥é©Ÿ1: åŠ è¼‰é è¨“ç·´æ¨¡å‹...")
    from model_setup import load_model_and_tokenizer
    model, tokenizer = load_model_and_tokenizer(device=device)
    
    # 2. æº–å‚™æ•¸æ“šé›†
    print("\nğŸ“Š æ­¥é©Ÿ2: æº–å‚™æ•¸æ“šé›†...")
    dataset = InterpretationDataset(csv_path, tokenizer, max_length=max_length)
    
    # åŠƒåˆ†è¨“ç·´é›†å’Œé©—è­‰é›†ï¼ˆ80/20ï¼‰
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    print(f"   è¨“ç·´é›†: {len(train_dataset)} æ¢")
    print(f"   é©—è­‰é›†: {len(val_dataset)} æ¢")
    
    # å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    
    # 3. è¨­ç½®å„ªåŒ–å™¨å’Œå­¸ç¿’ç‡èª¿åº¦å™¨
    print("\nâš™ï¸ æ­¥é©Ÿ3: è¨­ç½®å„ªåŒ–å™¨...")
    optimizer = AdamW(model.parameters(), lr=learning_rate)
    
    total_steps = len(train_loader) * num_epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps
    )
    
    print(f"   ç¸½è¨“ç·´æ­¥æ•¸: {total_steps}")
    print(f"   é ç†±æ­¥æ•¸: {warmup_steps}")
    
    # 4. è¨“ç·´å¾ªç’°
    print("\nğŸ¯ æ­¥é©Ÿ4: é–‹å§‹è¨“ç·´...")
    best_val_loss = float('inf')
    training_history = []
    
    for epoch in range(num_epochs):
        print(f"\n{'='*60}")
        print(f"ğŸ“… Epoch {epoch + 1}/{num_epochs}")
        print(f"{'='*60}")
        
        # è¨“ç·´éšæ®µ
        model.train()
        total_train_loss = 0
        
        progress_bar = tqdm(train_loader, desc=f"è¨“ç·´ Epoch {epoch + 1}")
        for batch_idx, batch in enumerate(progress_bar):
            # å°‡æ•¸æ“šç§»åˆ°è¨­å‚™
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            # å‰å‘å‚³æ’­
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            
            loss = outputs.loss
            
            # åå‘å‚³æ’­
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # æ¢¯åº¦è£å‰ª
            optimizer.step()
            scheduler.step()
            
            total_train_loss += loss.item()
            
            # æ›´æ–°é€²åº¦æ¢
            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        avg_train_loss = total_train_loss / len(train_loader)
        
        # é©—è­‰éšæ®µ
        print(f"\nğŸ” é©—è­‰éšæ®µ...")
        model.eval()
        total_val_loss = 0
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="é©—è­‰"):
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)
                
                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                
                total_val_loss += outputs.loss.item()
        
        avg_val_loss = total_val_loss / len(val_loader)
        
        # è¨ˆç®—å›°æƒ‘åº¦
        print(f"\nğŸ“Š è¨ˆç®—å›°æƒ‘åº¦...")
        perplexity = calculate_perplexity(model, val_loader, device)
        
        # è¨ˆç®—BLEUï¼ˆæ¯2å€‹epochè¨ˆç®—ä¸€æ¬¡ï¼Œå› ç‚ºæ¯”è¼ƒè€—æ™‚ï¼‰
        bleu_score = 0.0
        if (epoch + 1) % 2 == 0 or epoch == num_epochs - 1:
            print(f"ğŸ“Š è¨ˆç®—BLEUåˆ†æ•¸...")
            val_data = [dataset.data[i] for i in val_dataset.indices]
            bleu_score = calculate_bleu(model, tokenizer, val_data, device, num_samples=5)
        
        # è¨˜éŒ„è¨“ç·´æ­·å²
        epoch_history = {
            'epoch': epoch + 1,
            'train_loss': avg_train_loss,
            'val_loss': avg_val_loss,
            'perplexity': perplexity,
            'bleu': bleu_score
        }
        training_history.append(epoch_history)
        
        print(f"\nğŸ“ˆ Epoch {epoch + 1} çµæœ:")
        print(f"   è¨“ç·´æå¤±: {avg_train_loss:.4f}")
        print(f"   é©—è­‰æå¤±: {avg_val_loss:.4f}")
        print(f"   å›°æƒ‘åº¦: {perplexity:.2f}")
        if bleu_score > 0:
            print(f"   BLEUåˆ†æ•¸: {bleu_score:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            print(f"\nğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (é©—è­‰æå¤±: {avg_val_loss:.4f})...")
            
            os.makedirs(output_dir, exist_ok=True)
            model.save_pretrained(output_dir)
            tokenizer.save_pretrained(output_dir)
            
            # ä¿å­˜è¨“ç·´æ­·å²
            with open(os.path.join(output_dir, 'training_history.json'), 'w', encoding='utf-8') as f:
                json.dump(training_history, f, ensure_ascii=False, indent=2)
    
    print(f"\n{'='*60}")
    print("âœ… è¨“ç·´å®Œæˆï¼")
    print(f"{'='*60}")
    print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
    print(f"ğŸ“Š æœ€ä½³é©—è­‰æå¤±: {best_val_loss:.4f}")

if __name__ == "__main__":
    # è¨“ç·´åƒæ•¸
    train_model(
        csv_path='training_data_interpretation.csv',
        output_dir='models/interpretation_model',
        num_epochs=5,
        batch_size=4,
        learning_rate=5e-5,
        max_length=256,
        warmup_steps=100
    )

