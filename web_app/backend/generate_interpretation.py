"""
æ¨ç†æ¨¡å¡Šï¼šç”Ÿæˆè§£è®€æ–‡æœ¬
ç”¨æ–¼åŠ è¼‰è¨“ç·´å¥½çš„æ¨¡å‹ä¸¦ç”Ÿæˆç‰©ç¨®è§£è®€æ–‡æœ¬
"""

import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import os

class InterpretationGenerator:
    """
    è§£è®€æ–‡æœ¬ç”Ÿæˆå™¨
    """
    def __init__(self, model_path='models/interpretation_model', device='cpu'):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            model_path: è¨“ç·´å¥½çš„æ¨¡å‹è·¯å¾‘
            device: é‹è¡Œè¨­å‚™
        """
        self.device = torch.device(device)
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.loaded = False
    
    def load_model(self):
        """
        åŠ è¼‰è¨“ç·´å¥½çš„æ¨¡å‹
        """
        if self.loaded:
            return
        
        if not os.path.exists(self.model_path):
            print(f"âš ï¸ æ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨: {self.model_path}")
            print("ğŸ’¡ è«‹å…ˆè¨“ç·´æ¨¡å‹æˆ–æª¢æŸ¥æ¨¡å‹è·¯å¾‘")
            return False
        
        try:
            print(f"ğŸ“¥ æ­£åœ¨åŠ è¼‰æ¨¡å‹: {self.model_path}")
            self.tokenizer = GPT2Tokenizer.from_pretrained(self.model_path)
            self.model = GPT2LMHeadModel.from_pretrained(self.model_path)
            self.model.to(self.device)
            self.model.eval()
            self.loaded = True
            print(f"âœ… æ¨¡å‹åŠ è¼‰æˆåŠŸ (è¨­å‚™: {self.device})")
            return True
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è¼‰å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def generate(
        self,
        species,
        category,
        max_length=256,
        temperature=0.7,
        top_p=0.9,
        num_return_sequences=1
    ):
        """
        Generate interpretation text
        
        Args:
            species: Species name (e.g., "è‡ºç£è—éµ²" - Chinese name)
            category: Category (e.g., "fun_fact", "behavior", "habitat")
            max_length: Maximum generation length
            temperature: Temperature parameter (controls randomness, higher = more random)
            top_p: Nucleus sampling parameter (controls diversity)
            num_return_sequences: Number of sequences to generate
        
        Returns:
            generated_text: Generated interpretation text in English, None if failed
        """
        if not self.loaded:
            if not self.load_model():
                return None
        
        try:
            # Build input text (species name in Chinese, but output will be in English)
            input_text = f"{species}çš„{category}ï¼š"
            
            # Encode input
            input_ids = self.tokenizer.encode(input_text, return_tensors='pt').to(self.device)
            
            # Generate text
            with torch.no_grad():
                output = self.model.generate(
                    input_ids=input_ids,
                    max_length=max_length,
                    num_return_sequences=num_return_sequences,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.2  # Repetition penalty to avoid repetitive generation
                )
            
            # Decode generated text
            generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)
            
            # Remove input part, keep only generated part
            generated_text = generated_text.replace(input_text, "").strip()
            
            # Clean text (remove special characters and extra spaces)
            generated_text = generated_text.replace("\n", " ").strip()
            
            return generated_text
            
        except Exception as e:
            print(f"âŒ Generation failed: {e}")
            import traceback
            traceback.print_exc()
            return None

# å…¨å±€ç”Ÿæˆå™¨å¯¦ä¾‹ï¼ˆç”¨æ–¼å–®ä¾‹æ¨¡å¼ï¼‰
_global_generator = None

def get_generator(model_path='models/interpretation_model', device='cpu'):
    """
    ç²å–å…¨å±€ç”Ÿæˆå™¨å¯¦ä¾‹ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰
    
    Args:
        model_path: æ¨¡å‹è·¯å¾‘
        device: è¨­å‚™
    
    Returns:
        generator: ç”Ÿæˆå™¨å¯¦ä¾‹
    """
    global _global_generator
    
    if _global_generator is None:
        _global_generator = InterpretationGenerator(model_path, device)
        _global_generator.load_model()
    
    return _global_generator

if __name__ == "__main__":
    # æ¸¬è©¦ç”Ÿæˆ
    print("=" * 60)
    print("ğŸ§ª æ¸¬è©¦è§£è®€æ–‡æœ¬ç”Ÿæˆ")
    print("=" * 60)
    
    # å‰µå»ºç”Ÿæˆå™¨
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    generator = InterpretationGenerator(
        model_path='models/interpretation_model',
        device=device
    )
    
    # æ¸¬è©¦ç”Ÿæˆè‡ºç£è—éµ²çš„fun_fact
    print("\nğŸ“ æ¸¬è©¦ç”Ÿæˆï¼šè‡ºç£è—éµ²çš„fun_fact")
    print("-" * 60)
    
    result = generator.generate(
        species="è‡ºç£è—éµ²",
        category="fun_fact",
        max_length=256,
        temperature=0.7
    )
    
    if result:
        print(f"âœ… ç”ŸæˆæˆåŠŸï¼š")
        print(f"{result}")
    else:
        print("âŒ ç”Ÿæˆå¤±æ•—ï¼ˆå¯èƒ½æ˜¯æ¨¡å‹æœªè¨“ç·´æˆ–è·¯å¾‘éŒ¯èª¤ï¼‰")
        print("ğŸ’¡ æç¤ºï¼šè«‹å…ˆé‹è¡Œ train_interpretation_model.py è¨“ç·´æ¨¡å‹")

